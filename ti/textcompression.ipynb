{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Text Compression\n",
    "\n",
    "In this example we will analyze text compression using Huffman code and other codes as well. We will use texts downloaded from the Gutenberg database: Ulysses by James Joyce and The Complete Work of William Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ulysses.txt exists\n"
     ]
    }
   ],
   "source": [
    "ulysses=\"/tmp/ulysses.txt\"\n",
    "[ -f $ulysses ] && echo \"$ulysses exists\" || wget -q http://www.gutenberg.org/files/4300/4300-0.txt -O $ulysses\n",
    "shakespeare=\"/tmp/shakespeare.txt.gz\"\n",
    "[ -f shakespeare ] && echo \"$shakespeare exists\" || wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt -O $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Lets check the type of files we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ulysses.txt: UTF-8 Unicode (with BOM) text, with CRLF line terminators\n",
      "/tmp/shakespeare.txt.gz: gzip compressed data, was \"pg100.txt.utf8.gzip\", last modified: Sun Oct  1 05:16:45 2017, max compression, original size 5589889\n"
     ]
    }
   ],
   "source": [
    "file $ulysses\n",
    "file $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Note that ulysses.txt is encoded in UTF-8 and using BOM (byte order mark), which is not necessary. It also uses Windows line breaks using two symbols CR (carriage return) and LB (line break). \n",
    "\n",
    "\"The UTF-8 BOM is a sequence of bytes (EF BB BF) that allows the reader to identify a file as being encoded in UTF-8.\n",
    "Normally, the BOM is used to signal the endianness of an encoding, but since endianness is irrelevant to UTF-8, the BOM is unnecessary.\n",
    "According to the Unicode standard, the BOM for UTF-8 files is not recommended\"\n",
    "(https://stackoverflow.com/questions/2223882/whats-different-between-utf-8-and-utf-8-without-bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "tail --bytes=+4 $ulysses | tr -d '\\r' > ${ulysses%.txt}2.txt\n",
    "mv ${ulysses%.txt}2.txt $ulysses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Let's check again the file type and see it is cleaner as desired.\n",
    "We used **tail** to remove 16-bit code used as BOM and **tr** to remove cariage returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ulysses.txt: UTF-8 Unicode text\n"
     ]
    }
   ],
   "source": [
    "file $ulysses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Now we have a UTF-8 coded text. Not that it is not ASCII. It means the file has some non ASCII characters. Note that UTF-8 is compatible with ASCII (that means ASCII is a subset of UTF-8).\n",
    "\n",
    "Shakespeare file was compressed by gunzip. Let's decompress it and ckeck the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "gzip -d $shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/shakespeare.txt: UTF-8 Unicode (with BOM) text, with CRLF line terminators\n"
     ]
    }
   ],
   "source": [
    "shakespeare=${shakespeare%.gz}\n",
    "file $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "It has the same encoding ulysses.txt had. So we might do the same steps we did with ulysses.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/shakespeare.txt: ASCII text\n"
     ]
    }
   ],
   "source": [
    "tail --bytes=+4 $shakespeare | tr -d '\\r' > ${shakespeare%.txt}2.txt\n",
    "mv ${shakespeare%.txt}2.txt $shakespeare\n",
    "file $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "We have a pure ASCII file.\n",
    "\n",
    "Lets define a function to count the frequency of occurrence of characters. We are gonna convert the text to smallcaps and remove everything that is not [a-z\\n ]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'tmp': No such file or directory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cat $ulysses | tr -dc 'A-Za-z\\n ' | tr 'A-Z' 'a-z' | tr '\\n' ' ' | \n",
    "   tr -s ' ' > tmp && mv tmp $ulysses\n",
    "cat $shakespeare | tr -dc 'A-Za-z\\n ' | tr 'A-Z' 'a-z' | tr '\\n' ' ' | \n",
    "   tr -s ' ' > tmp && mv tmp $shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "charcount() {\n",
    "    FILENAME=\"$1\"\n",
    "    cat $FILENAME | sed 's/\\(.\\)/\\1\\n/g' | \n",
    "       sort | uniq -c | sort -n -r > \"${FILENAME}_charcount\"\n",
    "    cat \"${FILENAME}_charcount\" | tr -dc '0-9\\n' > \"${FILENAME}_ccount\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "charcount $ulysses\n",
    "charcount $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Now, let's check the result generated by the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 267159  \t  81154 n\t  33764 u\t  24597 y\t   1466 x\n",
      " 143271 e\t  77671 s\t  31891 m\t  22859 p\t   1342 q\n",
      " 101741 t\t  73078 h\t  30507 c\t  21433 b\t   1076 z\n",
      "  94117 a\t  71037 r\t  28220 g\t  12203 k\n",
      "  92726 o\t  55540 l\t  27010 f\t   9870 v\n",
      "  82481 i\t  49606 d\t  26453 w\t   2414 j\n"
     ]
    }
   ],
   "source": [
    "cat \"${ulysses}_charcount\" | column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To compute the entropy using a GNU Octave script directly from Bash, we created the script entropyfromdata.m bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/octave -qf\n",
      "\n",
      "arg_list = argv ();\n",
      "\n",
      "if( length(arg_list) == 0), error('no input file was given!'); endif;\n",
      "filename = arg_list{1};\n",
      "if( exist(filename,'file') != 2 ), error('file does not exist!'); endif;\n",
      "n = load(filename);\n",
      "if( size(n,2) != 1), error('a row vector file must be provided!'); endif;\n",
      "if( any(floor(n) != n) ), error('you must provide a vector of integers!'); endif\n",
      "\n",
      "\n",
      "if length( find(n==0,1) > 0), \n",
      "   n += ones(size(n)); % add one smothing\n",
      "endif\n",
      "\n",
      "% mle\n",
      "p = n./sum(n);\n",
      "H = entropy(p);\n",
      "printf('Entropy: %.2f bits\\n',H);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat entropyfromdata.m\n",
    "chmod +x entropyfromdata.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Bellow we use the script above to compute the entropy (bits/char) of both text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 4.12 bits\n"
     ]
    }
   ],
   "source": [
    "./entropyfromdata.m \"${ulysses}_ccount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 4.09 bits\n"
     ]
    }
   ],
   "source": [
    "./entropyfromdata.m \"${shakespeare}_ccount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "We have estimated an entropy of about 4.1 bits/char for English text. However, we know there is much interdependence between characters in a sequence, what was not grasped in previous estimation, where we have assumed a i.i.d. source of characters. \n",
    "\n",
    "To improve our entropy of English text estimation, we will consider now words as unities. We need to compute the word entropy and divite it by the average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "wordcount() {\n",
    "    FILENAME=\"$1\"\n",
    "    cat $FILENAME | tr ' ' '\\n' | sort | uniq -c | sort -n -r > \"${FILENAME}_wordcount\"\n",
    "    cat \"${FILENAME}_wordcount\" | tr -dc '0-9\\n' > \"${FILENAME}_wcount\"\n",
    "    cat \"${FILENAME}_wordcount\" | sed -E \"s/\\s*([0-9]+)\\s([a-z]+)$/\\1,\\2/\" > \"${FILENAME}_wcount.csv\"\n",
    "    echo \"count,word\" | cat - \"${FILENAME}_wcount.csv\" | sponge \"${FILENAME}_wcount.csv\"\n",
    "    # remove possible empty words\n",
    "    awk -F, -i inplace -v INPLACE_SUFFIX=.bak 'length($2) {print}' \"${FILENAME}_wcount.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "wordcount $ulysses\n",
    "wordcount $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Check the resulting file to see its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count,word\t5040,to\t\t2621,that\t1963,for\t1337,all\n",
      "15115,the\t4986,in\t\t2562,with\t1961,you\t1301,at\n",
      "8262,of\t\t4034,he\t\t2371,it\t\t1785,her\t1297,by\n",
      "7285,and\t3330,his\t2135,was\t1523,him\t1208,said\n",
      "6560,a\t\t2687,i\t\t2120,on\t\t1458,is\t\t1197,as\n"
     ]
    }
   ],
   "source": [
    "head -n 25 \"${ulysses}_wcount.csv\" | column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Now, lets compute the entropy (per word) of the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 10.62 bits\n"
     ]
    }
   ],
   "source": [
    "./entropyfromdata.m \"${ulysses}_wcount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 10.00 bits\n"
     ]
    }
   ],
   "source": [
    "./entropyfromdata.m \"${shakespeare}_wcount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "We need now to compute the average word length. It might be easily done with awk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.54308\n"
     ]
    }
   ],
   "source": [
    "awk -F, '{WLEN+=length($2)} END{print WLEN/NR}' \"${ulysses}_wcount.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.45483\n"
     ]
    }
   ],
   "source": [
    "awk -F, '{WLEN+=length($2)} END{print WLEN/NR}' \"${shakespeare}_wcount.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "We may now compute the entropy of the texts in bits/char.\n",
    "\n",
    "Ulysses:\n",
    "10.62 bits / 7.54308 chars = 1.41 bits/char\n",
    "\n",
    "Shakespeare:\n",
    "10.00 bits / 7.45483 chars = 1.34 bits/char\n",
    "\n",
    "\n",
    "If we compute the number of characters in these files and use the estimate entropy, we might estimate the theorical compression limit for these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464686 /tmp/ulysses.txt\n"
     ]
    }
   ],
   "source": [
    "wc -m $ulysses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4707386 /tmp/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "wc -m $shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "We shall have the following compression limits:\n",
    "\n",
    "* considering the character entropy for a source that produces i.i.d. characters \n",
    "  1. ulysses: 1464686*4.12 = 6034506 bits (754313 bytes);\n",
    "  2. shakespeare: 4707386*4.09 = 19253209 bits (2406651 bytes)\n",
    "* considering a source that produces words we have estimated a lower entropy per character\n",
    "  1. ulysses: 1464686*1.41 = 2065207 bits (258151 bytes);\n",
    "  2.  shakespeare: 4707386*1.34 = 6307897 bits (788487 bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To compress the files using different methods, we shall use the Basic Compression Library, available at http://bcl.comli.eu/ (alternatively here: https://github.com/MariadeAnton/bcl).\n",
    "\n",
    "We will use git to clone the repository locally and then compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/bcl'...\n",
      "remote: Enumerating objects: 143, done.\u001b[K\n",
      "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 143\u001b[K\n",
      "Receiving objects: 100% (143/143), 338.96 KiB | 350.00 KiB/s, done.\n",
      "Resolving deltas: 100% (46/46), done.\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/MariadeAnton/bcl /tmp/bcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -c -Wall -W -ansi -pedantic -O3 rle.c\n",
      "gcc -c -Wall -W -ansi -pedantic -O3 shannonfano.c\n",
      "gcc -c -Wall -W -ansi -pedantic -O3 huffman.c\n",
      "gcc -c -Wall -W -ansi -pedantic -O3 rice.c\n",
      "gcc -c -Wall -W -ansi -pedantic -O3 lz.c\n",
      "ar -rcs libbcl.a rle.o shannonfano.o huffman.o rice.o lz.o\n",
      "gcc -c -Wall -w -O3 -s bfc.c\n",
      "gcc -s bfc.o libbcl.a -o bfc.exe\n",
      "gcc -c -Wall -w -O3 -s bcltest.c\n",
      "gcc -c -Wall -w -O3 -s systimer.c\n",
      "gcc -s bcltest.o systimer.o libbcl.a -o bcltest.exe\n"
     ]
    }
   ],
   "source": [
    "CURRENTFOLDER=$(pwd)\n",
    "cd /tmp/bcl/src/\n",
    "make\n",
    "cd $CURRENTFOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Now that we have the executable available, lets compress the text files using Huffman and LZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffman compress /tmp/ulysses.txt to /tmp/ulysses.huf...\n",
      "Input file: 1464686 bytes\n",
      "Output file: 761928 bytes (52.0%)\n"
     ]
    }
   ],
   "source": [
    "/tmp/bcl/src/bfc.exe c huff $ulysses ${ulysses%txt}huf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffman compress /tmp/shakespeare.txt to /tmp/shakespeare.huf...\n",
      "Input file: 4707386 bytes\n",
      "Output file: 2434327 bytes (51.7%)\n"
     ]
    }
   ],
   "source": [
    "/tmp/bcl/src/bfc.exe c huff $shakespeare ${shakespeare%txt}huf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LZ77 compress /tmp/ulysses.txt to /tmp/ulysses.lz...\n",
      "Input file: 1464686 bytes\n",
      "Output file: 935970 bytes (63.9%)\n"
     ]
    }
   ],
   "source": [
    "/tmp/bcl/src/bfc.exe c lz $ulysses ${ulysses%txt}lz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LZ77 compress /tmp/shakespeare.txt to /tmp/shakespeare.lz...\n",
      "Input file: 4707386 bytes\n",
      "Output file: 2779499 bytes (59.0%)\n"
     ]
    }
   ],
   "source": [
    "/tmp/bcl/src/bfc.exe c lz $shakespeare ${shakespeare%txt}lz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "The table bellow presents the results achieved. \n",
    "\n",
    "obs.: the theoretical limits are for a source of characters model (c) or a source of words model (w).\n",
    "\n",
    "\n",
    "| compression           | ulysses     | shakespeare |\n",
    "|-----------------------|-------------|-------------|\n",
    "| no compression        |1464686 bytes|4707386 bytes|\n",
    "| theoretical limit (c) | 754313 bytes|2406651 bytes|\n",
    "| theoretical limit (w) | 258151 bytes| 788487 bytes|\n",
    "| Huffman               | 761928 bytes|2434327 bytes|\n",
    "| eficiency_huffman (c) |  0.990      |  0.989      |\n",
    "| eficiency_huffman (w) |  0.338      |  0.323      |\n",
    "| LZ                    | 935970 bytes|2779499 bytes|\n",
    "| eficiency_lz (c)      |  0.806      |  0.866      |\n",
    "| eficiency_lz (w)      |  0.276      |  0.284      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## GNU Octave\n",
    "\n",
    "Lets now use Huffman code in GNU Octave to compress the text file.\n",
    "\n",
    "Bellow is the code of a function that might be run from bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/octave -qf\n",
      "pkg load communications\n",
      "pkg load image\n",
      "\n",
      "% check input arguments\n",
      "arg_list = argv ();\n",
      "if( length(arg_list) == 0), error('no input file was given!'); endif;\n",
      "for i=1:length(arg_list),\n",
      "  if( exist(arg_list{i},'file') != 2 ), error('file does not exist!'); endif;\n",
      "end;\n",
      "% get filename\n",
      "if length(arg_list) == 1,\n",
      "  filename = arg_list{1};\n",
      "else error('too many inputs!');\n",
      "endif;\n",
      "\n",
      "text = fileread (filename);\n",
      "symbols = unique ( sort (text) );\n",
      "counts = arrayfun (@(x) sum(ismember(text,x)),symbols);\n",
      "[counts, id] = sort (counts,'descend');  \n",
      "symbols = symbols(id);\n",
      "\n",
      "p = counts./sum(counts); % mle of the distribution\n",
      "dict = huffmandict(symbols, p);\n",
      "% the function huffmanenco requeries integer index as input\n",
      "% therefore we will create a map to convert chars into integers\n",
      "% the mapped text will be used in the encoding method\n",
      "symbolmap = zeros(1,255);\n",
      "s2a = double (symbols); % convert to ascii values\n",
      "symbolmap(s2a([1:length(symbols)])) = [1:length(symbols)];\n",
      "out = huffmanenco(symbolmap( double (text) ), dict);\n",
      "\n",
      "% create a binarry representation of the output and \n",
      "% split it in blocks of 8 bits to be written to file\n",
      "binvector2dec = @(block_struct) bin2dec(sprintf('%d',block_struct));\n",
      "outdec = blockproc(out,[1 8],binvector2dec);\n",
      "\n",
      "% write file\n",
      "fid=fopen('/tmp/data.huf','w');\n",
      "fwrite(fid,uint8(outdec),'uint8');\n",
      "fclose(fid);\n",
      "\n",
      "save('-binary','/tmp/huffdic.huf','dict','symbols');\n",
      "system(cstrcat('cat /tmp/huffdic.huf > ',filename,'.huf'));\n",
      "system(cstrcat('echo -n \"###END_HD###\" >> ',filename,'.huf'));\n",
      "system(cstrcat('cat /tmp/data.huf >> ',filename,'.huf'));\n",
      "system('rm /tmp/huffdic.huf');\n",
      "system('rm tmp/data.huf');\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat huffmancompress.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leoca leoca 764529 out 11 12:40 /tmp/ulysses.txt.huf\n"
     ]
    }
   ],
   "source": [
    "chmod +x huffmancompress.m \n",
    "./huffmancompress.m $ulysses\n",
    "ls -l \"$ulysses.huf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "The file size is about the same as the file produced by the basic compression library. \n",
    "\n",
    "Decoding is performed by the function bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/octave -qf\n",
      "pkg load communications\n",
      "pkg load image\n",
      "\n",
      "% check input arguments\n",
      "arg_list = argv ();\n",
      "if( length(arg_list) == 0), error('no input file was given!'); endif;\n",
      "for i=1:length(arg_list),\n",
      "  if( exist(arg_list{i},'file') != 2 ), error('file does not exist!'); endif;\n",
      "end;\n",
      "% get filename\n",
      "if length(arg_list) == 1,\n",
      "  filename = arg_list{1};\n",
      "else error('too many inputs!');\n",
      "endif;\n",
      "\n",
      "cmd = cstrcat(\"SPLITPOS=$(grep -abo '###END_HD###' \",filename,\" | cut -d: -f1) && head -c $SPLITPOS \",filename,\" > /tmp/huffdic && tail -c +`expr $SPLITPOS + 13` \",filename,\" > /tmp/data\");\n",
      "system (cmd);\n",
      "load ('/tmp/huffdic');\n",
      "\n",
      "fid = fopen ('/tmp/data', 'r');\n",
      "x = fread (fid, Inf, 'uint8');\n",
      "fclose (fid);\n",
      "\n",
      "xbin = de2bi(x,8);\n",
      "xbin = xbin(:);\n",
      "back  = huffmandeco (xbin, dict);\n",
      "id = find (back == -1);\n",
      "back(id)=[];\n",
      "decode = char (symbols(back));\n",
      "\n",
      "fid = fopen(cstrcat(filename,'_decoded'),'w');\n",
      "fwrite (fid, decode, 'char');\n",
      "fclose(fid);\n"
     ]
    }
   ],
   "source": [
    "cat huffmandecompress.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "*obs: still not working properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.19.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
