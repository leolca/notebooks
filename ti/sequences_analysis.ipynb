{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Sequences in a text file\n",
    "\n",
    "We're gonna use two large texts to analyse the number of sequences of length n, what percentage of the total of possible sequences of length n it corresponds and entropy per character in the sequences.\n",
    "\n",
    "The texts are: *Ulysses* by James Joyce and *The History of a Young Lady* by Samuel Richardson. Both are available at the Gutenberg Project.\n",
    "\n",
    "To compute the entropy we are using the Python script available at [*clscripts*](https://github.com/leolca/clscripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -q https://www.gutenberg.org/files/4300/4300-0.txt -O /tmp/ulysses.txt\n",
    "wget -q https://github.com/leolca/clscripts/raw/master/entropy.py -O entropy.py\n",
    "chmod +x entropy.py\n",
    "wget -q https://github.com/leolca/clscripts/raw/master/ngram -O ngram\n",
    "chmod +x ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folloing script does the hard word. It receives two parameters: the text file name and the maximum sequence length we are going to analyse. \n",
    "\n",
    "We are considering a case insensitive approach, therefore we are converting uppercase into lowercase. We are interested in analyse sequeces maide only by of characters 'a-z'. \n",
    "\n",
    "In order to create sequences of length n, we could simply use *fold*. In this case we are not taking overlapping sequeces. See the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "def\n"
     ]
    }
   ],
   "source": [
    "echo \"abcdef\" | fold -w3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to get overlapping sequences, we need to use the *ngram* function from *clscripts*. See the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "bcd\n",
      "cde\n",
      "def\n"
     ]
    }
   ],
   "source": [
    "echo \"abcdef\" | ./ngram -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function also avoids (default behaviour) extracting n-grams acros word boundaries. Observe in the example bellow that the sequence 'met' and 'eta' (on the edge of the two words) are not generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim\n",
      "ime\n",
      "tab\n",
      "abl\n",
      "ble\n"
     ]
    }
   ],
   "source": [
    "echo \"time table\" | ./ngram -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For short sequences (small n), that might be desirable. If we keep this aproach for large values of n, we will get fewer sequences, since there are not many long words in a language. See bellow a simple example when I consider only sequences of length n=6 and n=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed\n",
      "change\n",
      "hanged\n"
     ]
    }
   ],
   "source": [
    "echo \"the train time table changed\" | tee >(./ngram -n 6) >(./ngram -n 7) > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also disconsider word boundaries by removing them. Using the simple 'time table' example, we observe that thet sequences 'met' and 'eta' are now generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim\n",
      "ime\n",
      "met\n",
      "eta\n",
      "tab\n",
      "abl\n",
      "ble\n"
     ]
    }
   ],
   "source": [
    "echo \"time table\" | tr -dc 'a-z' | ./ngram -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need not to list unique sequences and count the number of times each one appear. We're using GNU tools *sort* and *uniq*. The result will be used to 1) compute the entropy, 2) count the number of sequences observed, and 3) plot a graph using *gnuplot*. To do so we use *tee* to make pipes derivaions from the main stream. This creates the problem that each subshell might take a different amount of time to accomplish its job. To syncronize everything we used locks. The results are all saved in temporary files, which are gonna be read on the end to retrieve the computed values. Some bash have a bug on locks implementation, for that reason we also add a while loop to wait for non empty files. In the end we must assert that the temporary files were deleted, so we create a trap to be run regardless of the result of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "FILENAME=$1\n",
      "LIMIT=$2\n",
      "mkdir -p imgs\n",
      "printf \"%-20s%-20s%-20s%-20s%-20s\\n\" \"n\" \"entropy/char\" \"num_of_seq.\" \"%_of_total\" \"typ_set_size\"\n",
      "for i in `seq $LIMIT`\n",
      "do\n",
      "  tmpa=$(mktemp) tmpb=$(mktemp)\n",
      "  trap 'rm \"$tmpa\" \"$tmpb\"' EXIT\n",
      "  imgfilename=$(echo \"$FILENAME\" | cut -f 1 -d '.')`printf %03d $i`.png\n",
      "  cat $FILENAME |\n",
      "  (\n",
      "    flock 3\n",
      "    flock 4\n",
      "\n",
      "    #tr 'A-Z' 'a-z' | tr -dc 'a-z' | fold -w$i | sort |\n",
      "    #tr 'A-Z\\n' 'a-z ' | tr -dc 'a-z ' | ./ngram -n $i | sort |\n",
      "    tr 'A-Z' 'a-z' | tr -dc 'a-z' | ./ngram -n $i | sort |\n",
      "    uniq -c | sort -nr | awk '{print NR \"\\t\" $0}' |\n",
      "    tee >(\n",
      "        awk '{print $2}' | ./entropy.py | { sleep 0.2; cat; } > \"$tmpa\"\n",
      "        flock -u 3\n",
      "        ) >(\n",
      "        awk 'END{ print NR }' |  { sleep 0.3; cat; } > \"$tmpb\"\n",
      "        flock -u 4\n",
      "        ) >(\n",
      "        gnuplot -e \"set terminal png; set output '$imgfilename'; set xlabel 'rank'; set ylabel 'counts'; set title 'sequence length: $i'; set key left top; set logscale x 10; set logscale y 10; plot '-' using 1:2 with lines title 'text'\"\n",
      "        ) > /dev/null\n",
      "\n",
      "    (\n",
      "      flock 3\n",
      "      flock 4\n",
      "    ) 3<\"$tmpa\" 4<\"$tmpb\"\n",
      "  ) 3<\"$tmpa\" 4<\"$tmpb\"\n",
      "\n",
      "  while [ ! -s \"$tmpa\" ] || [ ! -s \"$tmpb\" ]; do sleep 0.5; done\n",
      "  ENTROPY=$(<\"$tmpa\")\n",
      "  NSEQ=$(<\"$tmpb\")\n",
      "  PERC=$(awk -v a=\"$NSEQ\" -v b=\"$i\" 'BEGIN {print a / (26 ** b)}')\n",
      "  ENTROPYC=$(awk -v a=\"$ENTROPY\" -v b=\"$i\" 'BEGIN {print a / b}')\n",
      "  TYPSETS=$(awk -v a=\"$ENTROPYC\" -v b=\"$i\" 'BEGIN {print 2 ** (b * a)}')\n",
      "  printf \"%-20s%-20s%-20s%-20s%-20s\\n\" \"$i\" \"$ENTROPYC\" \"$NSEQ\" \"$PERC\" \"$TYPSETS\"\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "cat sequenceanalysis.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As results, this script provides us with: 1) entropy per character; 2) number of sequences observed in the data for a ginve length n; 3) percentage of the total possible sequences of length n that were in fact observed in the data; 4) an approximation to the typical set size $2^{nH}$ . Observe that, as n gets large, the number of observed sequences approach the size of the typical set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                   entropy/char        num_of_seq.         %_of_total          typ_set_size        \n",
      "1                   4.2057              26                  1                   18.4519             \n",
      "2                   3.94375             634                 0.93787             236.796             \n",
      "3                   3.7291              8783                0.499716            2331.92             \n",
      "4                   3.5092              66114               0.144677            16807.3             \n",
      "5                   3.258               243472              0.0204919           80127               \n",
      "6                   2.97502             493099              0.00159622          236277              \n",
      "7                   2.69013             720894              8.97549e-05         466304              \n",
      "8                   2.42649             892221              4.27254e-06         697542              \n",
      "9                   2.19342             1006904             1.8545e-07          876127              \n",
      "10                  1.99217             1077620             7.63365e-09         993183              \n",
      "11                  1.82016             1119532             3.05021e-10         1.06451e+06         \n",
      "12                  1.67325             1144693             1.19952e-11         1.1076e+06          \n"
     ]
    }
   ],
   "source": [
    "./sequenceanalysis.sh /tmp/ulysses.txt 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage /tmp/ulysses*.png -tile 6x2 -geometry +0+0 /tmp/out.png 2>/dev/null\n",
    "convert /tmp/out.png -resize 800x ulysses_results.png\n",
    "rm /tmp/out.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ulysses](ulysses_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                   entropy/char        num_of_seq.         %_of_total          typ_set_size        \n",
      "1                   4.1784              26                  1                   18.1061             \n",
      "2                   3.8923              617                 0.912722            220.495             \n",
      "3                   3.62935             7900                0.449477            1895.09             \n",
      "4                   3.36259             59541               0.130293            11193.4             \n",
      "5                   3.10821             250821              0.0211104           47678.5             \n",
      "6                   2.87263             641015              0.00207505          154343              \n",
      "7                   2.6533              1180888             0.000147026         389996              \n",
      "8                   2.44795             1770226             8.477e-06           785690              \n",
      "9                   2.25703             2317249             4.26788e-07         1.30288e+06         \n",
      "10                  2.0822              2774635             1.9655e-08          1.85373e+06         \n",
      "11                  1.92423             3128843             8.52466e-10         2.35375e+06         \n",
      "12                  1.78295             3390368             3.55277e-11         2.7584e+06          \n"
     ]
    }
   ],
   "source": [
    "wget -nc -q https://www.gutenberg.org/ebooks/9296.txt.utf-8 -O /tmp/harlowe001.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/9798.txt.utf-8 -O /tmp/harlowe002.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/9881.txt.utf-8 -O /tmp/harlowe003.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/10462.txt.utf-8 -O /tmp/harlowe004.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/10799.txt.utf-8 -O /tmp/harlowe005.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/11364.txt.utf-8 -O /tmp/harlowe006.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/11889.txt.utf-8 -O /tmp/harlowe007.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/12180.txt.utf-8 -O /tmp/harlowe008.txt\n",
    "wget -nc -q https://www.gutenberg.org/ebooks/12398.txt.utf-8 -O /tmp/harlowe009.txt\n",
    "cat /tmp/harlowe0*.txt > /tmp/harlowe_history_of_a_young_lady.txt\n",
    "./sequenceanalysis.sh /tmp/harlowe_history_of_a_young_lady.txt 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage /tmp/harlowe_*.png -tile 6x2 -geometry +0+0 /tmp/out.png 2>/dev/null\n",
    "convert /tmp/out.png -resize 800x harlowe_results.png\n",
    "rm /tmp/out.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![harlowe](harlowe_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type might be undestood as the empirical histogram. Strings that have the same empirical histogram are strings of the same type. For example: 'opts', 'post', 'pots', 'spot', 'stop' and 'tops' share the same type, they all have 1 o, 1 p, 1 s and 1 t. Words that are anagram have the type! Just for fun, we could list them. We're gonna use the awk script available at *clscripts* (this script was written by Arnold Robbins, based on the algorithm from Jon Bentley)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -q https://github.com/leolca/clscripts/raw/master/anagram.awk -O anagram.awk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of words that are anagrams and start with q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuads squad \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuakes squeak \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuartets squatter \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuids squid \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuieter requite \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuiet quite \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuires squire \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuotes toques \n",
      "\u001b[01;31m\u001b[Kq\u001b[m\u001b[Kuote toque \n"
     ]
    }
   ],
   "source": [
    "awk -f anagram.awk /usr/share/dict/words | grep '^q'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a list of the words that have more than 5 anagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abets baste bates beast beats betas \n",
      "aster rates stare tares taser tears \n",
      "caret cater crate react recta trace \n",
      "carets caster caters crates reacts recast traces \n",
      "drapes padres parsed rasped spared spread \n",
      "lapse leaps pales peals pleas sepal \n",
      "least slate stale steal tales teals \n",
      "opts post pots spot stop tops \n",
      "palest pastel petals plates pleats staple \n",
      "pares parse pears rapes reaps spare spear \n"
     ]
    }
   ],
   "source": [
    "awk -f anagram.awk /usr/share/dict/words | awk 'NF > 5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the type of a string we are gonna use the script *type.sh* from *clscripts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1s1t2\n"
     ]
    }
   ],
   "source": [
    "wget -q https://github.com/leolca/clscripts/raw/master/type -O type\n",
    "chmod +x type\n",
    "echo \"test\" | ./type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute the type of the anagrams we listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\ttype\n",
      "abets\ta1b1e1s1t1\n",
      "baste\ta1b1e1s1t1\n",
      "bates\ta1b1e1s1t1\n",
      "beast\ta1b1e1s1t1\n",
      "beats\ta1b1e1s1t1\n",
      "betas\ta1b1e1s1t1\n",
      "aster\ta1e1r1s1t1\n",
      "rates\ta1e1r1s1t1\n",
      "stare\ta1e1r1s1t1\n",
      "tares\ta1e1r1s1t1\n",
      "taser\ta1e1r1s1t1\n",
      "tears\ta1e1r1s1t1\n",
      "caret\ta1c1e1r1t1\n",
      "cater\ta1c1e1r1t1\n",
      "crate\ta1c1e1r1t1\n",
      "react\ta1c1e1r1t1\n",
      "recta\ta1c1e1r1t1\n",
      "trace\ta1c1e1r1t1\n",
      "carets\ta1c1e1r1s1t1\n",
      "caster\ta1c1e1r1s1t1\n",
      "caters\ta1c1e1r1s1t1\n",
      "crates\ta1c1e1r1s1t1\n",
      "reacts\ta1c1e1r1s1t1\n",
      "recast\ta1c1e1r1s1t1\n",
      "traces\ta1c1e1r1s1t1\n",
      "drapes\ta1d1e1p1r1s1\n",
      "padres\ta1d1e1p1r1s1\n",
      "parsed\ta1d1e1p1r1s1\n",
      "rasped\ta1d1e1p1r1s1\n",
      "spared\ta1d1e1p1r1s1\n",
      "spread\ta1d1e1p1r1s1\n",
      "lapse\ta1e1l1p1s1\n",
      "leaps\ta1e1l1p1s1\n",
      "pales\ta1e1l1p1s1\n",
      "peals\ta1e1l1p1s1\n",
      "pleas\ta1e1l1p1s1\n",
      "sepal\ta1e1l1p1s1\n",
      "least\ta1e1l1s1t1\n",
      "slate\ta1e1l1s1t1\n",
      "stale\ta1e1l1s1t1\n",
      "steal\ta1e1l1s1t1\n",
      "tales\ta1e1l1s1t1\n",
      "teals\ta1e1l1s1t1\n",
      "opts\to1p1s1t1\n",
      "post\to1p1s1t1\n",
      "pots\to1p1s1t1\n",
      "spot\to1p1s1t1\n",
      "stop\to1p1s1t1\n",
      "tops\to1p1s1t1\n",
      "palest\ta1e1l1p1s1t1\n",
      "pastel\ta1e1l1p1s1t1\n",
      "petals\ta1e1l1p1s1t1\n",
      "plates\ta1e1l1p1s1t1\n",
      "pleats\ta1e1l1p1s1t1\n",
      "staple\ta1e1l1p1s1t1\n",
      "pares\ta1e1p1r1s1\n",
      "parse\ta1e1p1r1s1\n",
      "pears\ta1e1p1r1s1\n",
      "rapes\ta1e1p1r1s1\n",
      "reaps\ta1e1p1r1s1\n",
      "spare\ta1e1p1r1s1\n",
      "spear\ta1e1p1r1s1\n"
     ]
    }
   ],
   "source": [
    "echo -e \"word\\ttype\"\n",
    "mkfifo /tmp/myfifo\n",
    "awk -f anagram.awk /usr/share/dict/words | awk 'BEGIN{OFS=\"\\n\"} NF > 5 {$1=$1; print}' | \n",
    "   tee >(./type > /tmp/myfifo) | paste - /tmp/myfifo \n",
    "rm /tmp/myfifo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets modify the previous script so that now we also compute the number of types observed in a text file. We just need to add a new `tee` and get one the stream to compute the types and the other stream to do the same computations as done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "FILENAME=$1\n",
      "LIMIT=$2\n",
      "mkdir -p imgs\n",
      "printf \"%-20s%-20s%-20s%-20s%-20s%-20s\\n\" \"n\" \"entropy/char\" \"num_of_seq.\" \"%_of_total\" \"num_types\" \"typ_set_size\"\n",
      "for i in `seq $LIMIT`\n",
      "do\n",
      "  tmpa=$(mktemp) tmpb=$(mktemp) tmpc=$(mktemp)\n",
      "  trap 'rm \"$tmpa\" \"$tmpb\" \"$tmpc\"' EXIT\n",
      "  imgfilename=$(echo \"$FILENAME\" | cut -f 1 -d '.')`printf %03d $i`.png\n",
      "  cat $FILENAME |\n",
      "  (\n",
      "    flock 3\n",
      "    flock 4\n",
      "    flock 5\n",
      "\n",
      "    #tr 'A-Z' 'a-z' | tr -dc 'a-z' | fold -w$i | tee >(\n",
      "    #tr 'A-Z\\n' 'a-z ' | tr -dc 'a-z ' | ./ngram -n $i | tee >(\n",
      "    tr 'A-Z' 'a-z' | tr -dc 'a-z' | ./ngram -n $i | tee >(\n",
      "       ./type | sort | uniq | awk 'END{ print NR }' > \"$tmpc\"\n",
      "       flock -u 5\n",
      "       ) | sort | uniq -c | sort -nr | awk '{print NR \"\\t\" $0}' |\n",
      "    tee >(\n",
      "        awk '{print $2}' | ./entropy.py > \"$tmpa\"\n",
      "        flock -u 3\n",
      "        ) >(\n",
      "        awk 'END{ print NR }' > \"$tmpb\"\n",
      "        flock -u 4\n",
      "        ) >(\n",
      "        gnuplot -e \"set terminal png; set output '$imgfilename'; set xlabel 'rank'; set ylabel 'counts'; set title 'sequence length: $i'; set key left top; set logscale x 10; set logscale y 10; plot '-' using 1:2 with lines title 'text'\"\n",
      "        ) > /dev/null\n",
      "\n",
      "    (\n",
      "      flock 3\n",
      "      flock 4\n",
      "      flock 5\n",
      "    ) 3<\"$tmpa\" 4<\"$tmpb\" 5<\"$tmpc\"\n",
      "  ) 3<\"$tmpa\" 4<\"$tmpb\" 5<\"$tmpc\"\n",
      "\n",
      "  while [ ! -s \"$tmpa\" ] || [ ! -s \"$tmpb\" ] || [ ! -s \"$tmpc\" ]; do sleep 0.5; done\n",
      "  ENTROPY=$(<\"$tmpa\")\n",
      "  NSEQ=$(<\"$tmpb\")\n",
      "  NTYPES=$(<\"$tmpc\")\n",
      "  PERC=$(awk -v a=\"$NSEQ\" -v b=\"$i\" 'BEGIN {print a / (26 ** b)}')\n",
      "  ENTROPYC=$(awk -v a=\"$ENTROPY\" -v b=\"$i\" 'BEGIN {print a / b}')\n",
      "  TYPSETS=$(awk -v a=\"$ENTROPYC\" -v b=\"$i\" 'BEGIN {print 2 ** (b * a)}')\n",
      "  printf \"%-20s%-20s%-20s%-20s%-20s%-20s\\n\" \"$i\" \"$ENTROPYC\" \"$NSEQ\" \"$PERC\" \"$NTYPES\" \"$TYPSETS\"\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "cat typeanalysis.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n                   entropy/char        num_of_seq.         %_of_total          num_types           typ_set_size        \n",
      "1                   4.2057              26                  1                   26                  18.4519             \n",
      "2                   3.94375             634                 0.93787             344                 236.796             \n",
      "3                   3.7291              8783                0.499716            2505                2331.92             \n",
      "4                   3.5092              66114               0.144677            11821               16807.3             \n",
      "5                   3.258               243472              0.0204919           41253               80127               \n",
      "6                   2.97502             493099              0.00159622          108987              236277              \n",
      "7                   2.69013             720894              8.97549e-05         227045              466304              \n",
      "8                   2.42649             892221              4.27254e-06         387677              697542              \n",
      "9                   2.19342             1006904             1.8545e-07          563321              876127              \n",
      "10                  1.99217             1077620             7.63365e-09         724535              993183              \n",
      "11                  1.82016             1119532             3.05021e-10         853467              1.06451e+06         \n",
      "12                  1.67325             1144693             1.19952e-11         945852              1.1076e+06          \n"
     ]
    }
   ],
   "source": [
    "./typeanalysis.sh /tmp/ulysses.txt 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
